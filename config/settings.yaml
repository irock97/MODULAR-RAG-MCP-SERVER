 
# Modular RAG MCP Server - Configuration
# This is the main configuration file for the MCP Server.
# Edit this file to configure LLM, Embedding, VectorStore, and other services.

# =============================================================================
# LLM Configuration
# =============================================================================
llm:
  provider: "openai"  # Options: openai, azure, ollama, deepseek
  model: "gpt-4o-mini"
  temperature: 0.0
  max_tokens: 4096
  # Provider-specific settings (uncomment as needed)
  # api_key: ${OPENAI_API_KEY}  # Use environment variable
  # base_url: "https://api.openai.com/v1"

# =============================================================================
# Embedding Configuration
# =============================================================================
embedding:
  provider: "openai"  # Options: openai, local
  model: "text-embedding-3-small"
  dimensions: 1536
  # api_key: ${OPENAI_API_KEY}

# =============================================================================
# Vector Store Configuration
# =============================================================================
vector_store:
  provider: "chroma"  # Options: chroma, qdrant, pinecone
  persist_directory: "./data/db/chroma"
  collection_name: "knowledge_hub"

# =============================================================================
# Retrieval Configuration
# =============================================================================
retrieval:
  dense_top_k: 20
  sparse_top_k: 20
  fusion_top_k: 10
  rrf_k: 60  # RRF constant

# =============================================================================
# Rerank Configuration
# =============================================================================
rerank:
  enabled: false
  provider: "none"  # Options: none, cross_encoder, llm
  model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  top_k: 5

# =============================================================================
# Evaluation Configuration
# =============================================================================
evaluation:
  enabled: false
  provider: "custom"  # Options: ragas, deepeval, custom
  metrics:
    - "hit_rate"
    - "mrr"
    - "faithfulness"

# =============================================================================
# Observability Configuration
# =============================================================================
observability:
  log_level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR
  trace_enabled: true
  trace_file: "./logs/traces.jsonl"
  structured_logging: true

# =============================================================================
# Ingestion Configuration
# =============================================================================
ingestion:
  chunk_size: 1000
  chunk_overlap: 200
  splitter: "recursive"  # Options: recursive, semantic, fixed_length
  batch_size: 100


# =============================================================================
# Vision LLM Configuration (for Image Captioning)
# =============================================================================
vision_llm:
  enabled: true  # Set to true to enable vision capabilities
  provider: "qwen"  # Options: qwen, azure, openai, ollama
  model: "qwen-vl-max"  # Options: qwen-vl-max, qwen-vl-plus (Qwen), gpt-4o (OpenAI/Azure)
  max_image_size: 2048  # Max dimension (width/height) for image compression
  timeout: 60  # Request timeout in seconds

  # OpenAI-specific (if provider is openai):
  # api_key: ${OPENAI_API_KEY}

  # Azure-specific (if provider is azure):
  # api_key: ${AZURE_OPENAI_API_KEY}
  # azure_endpoint: ""  # e.g., https://my-resource.openai.azure.com/
  # deployment_name: "gpt-4o"  # Deployment name
  # api_version: "2024-02-15-preview"

  # Qwen-specific (if provider is qwen):
#  api_key: {}
#  base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"  # API endpoint

  # Ollama-specific (if provider is ollama):
  # base_url: "http://localhost:11434"